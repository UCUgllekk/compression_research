{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compression algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"\n",
    "    Class Node for Huffman and LZ77 algorithms.\n",
    "    \"\"\"\n",
    "    def __init__(self, freq: bytes, char=None, offset=None, length=0, next_byte=None):\n",
    "        \"\"\"\n",
    "        Initializes a node with relevant attributes.\n",
    "        \"\"\"\n",
    "        self.freq = freq\n",
    "        self.char = char\n",
    "        self.offset = offset\n",
    "        self.length = length\n",
    "        self.next_byte = next_byte\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        String representation of the node.\n",
    "        \"\"\"\n",
    "        if self.char is not None:\n",
    "            return f'(\"{self.char}\", {self.freq})'\n",
    "        else:\n",
    "            return f'(Offset: {self.offset}, Length: {self.length}, Next Byte: {self.next_byte})'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuffmanCompression:\n",
    "    \"\"\"\n",
    "    Class for compression and decompression, uses\n",
    "    Huffman algorithm.\n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Initializes dict of symbols and name of algorithm\n",
    "        \"\"\"\n",
    "        self.main_dict = {}\n",
    "        self.name = 'huff'\n",
    "\n",
    "    def generate_code(self, node: 'Node', coding: str) -> None:\n",
    "        \"\"\"\n",
    "        Generates all codings of symbols\n",
    "        \"\"\"\n",
    "        if node.char is not None:\n",
    "            char = node.char\n",
    "            code = coding\n",
    "            self.main_dict[char] = code\n",
    "        else:\n",
    "            self.generate_code(node.left_child, coding + '0')\n",
    "            self.generate_code(node.right_child, coding + '1')\n",
    "\n",
    "    def encode(self, text: str) -> tuple[str, dict[bytes, str]]:\n",
    "        \"\"\"\n",
    "        Encoder functions, returns sequence of bits and dict of all codings.\n",
    "        \"\"\"\n",
    "        frequency = self.frequency(text)\n",
    "        nodes = []\n",
    "        for (char, freq) in frequency:\n",
    "            nodes.append(Node(freq=freq, char=char))\n",
    "        while len(nodes) > 1:\n",
    "            first_lowest = nodes[0]\n",
    "            second_lowest = nodes[1]\n",
    "            min_freq = nodes.pop(0).freq + nodes.pop(0).freq\n",
    "            new_node = Node(freq=min_freq)\n",
    "            new_node.left_child = first_lowest\n",
    "            new_node.right_child = second_lowest\n",
    "            nodes.append(new_node)\n",
    "            nodes = sorted(nodes, key= lambda x: x.freq)\n",
    "        self.generate_code(nodes[0], '')\n",
    "        coded_str = ''\n",
    "        for i in text:\n",
    "            coded_str += self.main_dict[i]\n",
    "        return (coded_str, self.main_dict)\n",
    "\n",
    "    def frequency(self, text: bytes):\n",
    "        \"\"\"\n",
    "        Function that helps calculate the frequency of all symbols\n",
    "        \"\"\"\n",
    "        freq = {}\n",
    "        for char in text:\n",
    "            if char not in freq:\n",
    "                freq[char] = 1\n",
    "            else:\n",
    "                freq[char] += 1\n",
    "        return sorted(freq.items(), key= lambda x: x[1])\n",
    "\n",
    "    def decode(self, code: str, coding_dict: dict[bytes, str]) -> bytes:\n",
    "        \"\"\"\n",
    "        Decodes files\n",
    "        \"\"\"\n",
    "        decoded_str = bytearray()\n",
    "        coding_dict = {i : j for j, i in coding_dict.items()}\n",
    "        buffer = ''\n",
    "        for bit in code:\n",
    "            buffer += bit\n",
    "            if buffer in coding_dict:\n",
    "                decoded_str += bytes([coding_dict[buffer]])\n",
    "                buffer = ''\n",
    "        return decoded_str\n",
    "\n",
    "    def add_fictious_bins(self, bin_str: str) -> str:\n",
    "        \"\"\"\n",
    "        Adds fictious bits in the end, so that sequence\n",
    "        can be fully transformed into bytes. Also adds\n",
    "        byte representaion of nums of added bits, so that\n",
    "        correct number will be removed in decompression\n",
    "        \"\"\"\n",
    "        fictious = 8 - len(bin_str) % 8\n",
    "        for i in range(fictious):\n",
    "            bin_str += '0'\n",
    "        fictious_info_secret = \"{0:08b}\".format(fictious)\n",
    "        return fictious_info_secret + bin_str\n",
    "\n",
    "    def remove_fictious(self, bin_str: str):\n",
    "        \"\"\"\n",
    "        Removes fictious bits\n",
    "        \"\"\"\n",
    "        fict_info = bin_str[:8]\n",
    "        fict_info = int(fict_info, 2)\n",
    "        bin_str = bin_str[8:]\n",
    "        return bin_str[:-1 * fict_info]\n",
    "\n",
    "    def dict_to_bytes(self, dictt: dict):\n",
    "        \"\"\"\n",
    "        Converts dicts to bytes\n",
    "        \"\"\"\n",
    "        keys = bytes(dictt.keys())\n",
    "        values = bytes(':'.join(dictt.values()), encoding='utf-8')\n",
    "        data = keys + b'separ' + values + b'end'\n",
    "        return data\n",
    "\n",
    "    def dict_from_bytes(self, data: bytes):\n",
    "        \"\"\"\n",
    "        Converts dict(expressed by bytes) to dict\n",
    "        \"\"\"\n",
    "        data = data.split(b'separ')\n",
    "        keys = list(data[0])\n",
    "        values = list(data[1].decode('utf-8').split(':'))\n",
    "        reconstructed_dict = {}\n",
    "        for i in range(len(keys)):\n",
    "            key = keys[i]\n",
    "            value = values[i]\n",
    "            reconstructed_dict[key] = value\n",
    "        return reconstructed_dict\n",
    "\n",
    "    def compress(self, path: str) -> None:\n",
    "        \"\"\"\n",
    "        Compresses files, uses encode func to generate\n",
    "        sequence of bits\n",
    "        \"\"\"\n",
    "        with open(path, 'rb') as file:\n",
    "            image = file.read()\n",
    "        encoded_data, encoded_dict = self.encode(image)\n",
    "        encoded_data = self.add_fictious_bins(encoded_data)\n",
    "        b = bytearray()\n",
    "        for i in range(0, len(encoded_data), 8):\n",
    "            byte = encoded_data[i:i+8]\n",
    "            b.append(int(byte, 2))\n",
    "        new_path = f'{path}.{self.name}'\n",
    "        with open(new_path, 'wb') as file:\n",
    "            file.write(self.dict_to_bytes(encoded_dict))\n",
    "            file.write(bytes(b))\n",
    "        return new_path\n",
    "\n",
    "    def decompress(self, file: tuple[str, dict, str]) -> None:\n",
    "        \"\"\"\n",
    "        Removes fictious bits and decompresses files.\n",
    "        \"\"\"\n",
    "        o_u = os.path.splitext(file)\n",
    "        f_f = os.path.splitext(o_u[0])\n",
    "        output_path = f_f[0] + '_decoded' + f_f[1]\n",
    "        with open(file, 'rb') as file:\n",
    "            file = file.read()\n",
    "            encoded_dict = file[:file.index(b'end')]\n",
    "            file = file[file.index(b'end')+3:]\n",
    "            encoded_dict = self.dict_from_bytes(encoded_dict)\n",
    "            bit_str = ''\n",
    "            for byte in file:\n",
    "                bits = bin(byte)[2:].rjust(8, '0')\n",
    "                bit_str += bits\n",
    "            bit_str = self.remove_fictious(bit_str)\n",
    "            decompr = self.decode(bit_str, encoded_dict)\n",
    "        with open(output_path, 'wb') as writte:\n",
    "            writte.write(bytes(decompr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LZ77"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LZ77:\n",
    "    def __init__(self, buffer_size: int):\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "    def find_best_match(self, cur_ind: int, data: str) -> Node:\n",
    "        buffer = data[max(0, cur_ind - self.buffer_size):cur_ind]\n",
    "        if data[cur_ind] not in buffer:\n",
    "            return Node(freq=0, char=data[cur_ind])\n",
    "\n",
    "        possib = [ind for ind, elem in enumerate(buffer) if elem == data[cur_ind]]\n",
    "        possib = [cur_ind - len(buffer) + elem for elem in possib]\n",
    "\n",
    "        best_match = Node(freq=0, char=data[cur_ind])\n",
    "        for offset in possib:\n",
    "            length = 0\n",
    "            while cur_ind + length < len(data) and data[cur_ind + length] == data[offset + length]:\n",
    "                length += 1\n",
    "                if len(buffer) == self.buffer_size:\n",
    "                    buffer = buffer[1:] + data[cur_ind + length - 1]\n",
    "                else:\n",
    "                    buffer += data[cur_ind + length - 1]\n",
    "\n",
    "            if cur_ind + length == len(data):\n",
    "                char = data[-1]\n",
    "                best_match = Node(freq=length + 1, offset=offset, next_byte=char) if char == data[offset + length - 1] else Node(freq=length, offset=offset, next_byte=char)\n",
    "                break\n",
    "            elif length >= best_match.length:\n",
    "                best_match = Node(freq=length, offset=offset, next_byte=data[cur_ind + length])\n",
    "\n",
    "        return best_match\n",
    "\n",
    "    def encode_string(self, data: str) -> list[Node]:\n",
    "        result = []\n",
    "        cur_ind = 0\n",
    "        while cur_ind < len(data):\n",
    "            match = self.find_best_match(cur_ind, data)\n",
    "            if match is None or match.length == 0:\n",
    "                result.append(Node(freq=1, char=data[cur_ind]))\n",
    "                cur_ind += 1\n",
    "            else:\n",
    "                result.append(match)\n",
    "                cur_ind += match.length\n",
    "        return result\n",
    "\n",
    "    def decode_string(self, code: list[Node]) -> str:\n",
    "        result = []\n",
    "        decoded_data = \"\"\n",
    "        for node in code:\n",
    "            if node.char is not None:\n",
    "                result.append(node.char)\n",
    "                decoded_data += node.char\n",
    "            else:\n",
    "                for _ in range(node.length):\n",
    "                    result.append(result[-node.offset])\n",
    "                    decoded_data += result[-node.offset]\n",
    "        return decoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LZ78"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lz78(data):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LZW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LZW:\n",
    "    '''LZW compression algorithm'''\n",
    "    name = 'lzw'\n",
    "    @staticmethod\n",
    "    def encoding(data: bytes) -> list:\n",
    "        '''LZW encoding'''\n",
    "        output = []\n",
    "        w = b''\n",
    "        dictionary = {bytes([i]): i for i in range(256)}\n",
    "        for byte in data:\n",
    "            byte = bytes([byte])\n",
    "            wc = w + byte\n",
    "            if wc in dictionary:\n",
    "                w = wc\n",
    "            else:\n",
    "                output.append(dictionary[w])\n",
    "                dictionary[wc] = len(dictionary)\n",
    "                w = byte\n",
    "        if w:\n",
    "            output.append(dictionary[w])\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def decoding(code: list) -> bytes:\n",
    "        '''LZW decoding'''\n",
    "        coding_dict = {i:bytes([i]) for i in range(256)}\n",
    "        string = coding_dict[code[0]]\n",
    "        output = bytearray()\n",
    "        output += string\n",
    "        for i in range(1, len(code)):\n",
    "            new = code[i]\n",
    "            if new not in coding_dict:\n",
    "                entry = string + string[:1]\n",
    "            else:\n",
    "                entry = coding_dict[new]\n",
    "            output += entry\n",
    "            coding_dict[len(coding_dict)] = string + entry[:1]\n",
    "            string = entry\n",
    "        return bytes(output)\n",
    "\n",
    "    @staticmethod\n",
    "    def compress(path:str):\n",
    "        '''Encode and write to file'''\n",
    "        with open(path, 'rb') as file:\n",
    "            data = file.read()\n",
    "        encoded_data = LZW.encoding(data)\n",
    "        file_path = path + '.' + LZW.name.lower()\n",
    "        with open(file_path, 'wb') as file:\n",
    "            for value in encoded_data:\n",
    "                file.write(value.to_bytes(3, byteorder='little'))\n",
    "        return file_path\n",
    "\n",
    "    @staticmethod\n",
    "    def decompress(path:str):\n",
    "        '''Read, decode, write to file'''\n",
    "        with open(path, 'rb') as file:\n",
    "            encoded_data = []\n",
    "            while (byte := file.read(3)):\n",
    "                encoded_data.append(int.from_bytes(byte, byteorder='little'))\n",
    "        decoded = LZW.decoding(encoded_data)\n",
    "        file_path = path[:-4][::-1].split('.', maxsplit=1)\n",
    "        file_path = '.'.join([file_path[1][::-1] + \"_decoded\", file_path[0][::-1]])\n",
    "        with open(file_path, 'wb') as file:\n",
    "            file.write(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deflate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Deflate:\n",
    "    def __init__(self, buffer_size: int):\n",
    "        self.huffman = Huffman_Compression()\n",
    "        self.lz77 = LZ77(buffer_size)\n",
    "\n",
    "    def dict_to_bytes(self, dictt):\n",
    "        keys = bytes(dictt.keys())\n",
    "        values = bytes(':'.join(dictt.values()), encoding='utf-8')\n",
    "        data = keys + b'|' + values\n",
    "        return data\n",
    "\n",
    "    def dict_from_bytes(self, data):\n",
    "        data = data.split(b'|')\n",
    "        keys = list(data[0])\n",
    "        values = list(data[1].decode('utf-8').split(':'))\n",
    "        reconstructed_dict = {}\n",
    "        for i in range(len(keys)):\n",
    "            key = keys[i]\n",
    "            value = values[i]\n",
    "            reconstructed_dict[key] = value.replace('_', '')\n",
    "        return reconstructed_dict\n",
    "\n",
    "    def compress(self, file_path: str) -> bytes:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            data = file.read()\n",
    "        \n",
    "        compressed_data = self.deflate(data)\n",
    "\n",
    "        # Save compressed data to a new file with .deflate extension\n",
    "        compressed_file_path = file_path + '.deflate'\n",
    "        with open(compressed_file_path, 'wb') as file:\n",
    "            pickle.dump(compressed_data[1], file)\n",
    "            file.write(self.dict_to_bytes(compressed_data[0]))\n",
    "\n",
    "    def decompress(self, compressed_file_path: str) -> str:\n",
    "        with open(compressed_file_path, 'rb') as file:\n",
    "            compressed_lz77_nodes = pickle.load(file)\n",
    "            compressed_dict = self.dict_from_bytes(file.readline().rstrip())\n",
    "        out_file_name = compressed_file_path.replace('.deflate', '')\n",
    "        \n",
    "        # Step 1: Huffman decoding\n",
    "        huffman_dict = compressed_dict\n",
    "        # Step 2: LZ77 decoding\n",
    "        huffman_encoded = self.lz77.decode_string(compressed_lz77_nodes)\n",
    "        # Step 3: Huffman decoding\n",
    "        decoded_text = self.huffman.decode(huffman_encoded, huffman_dict)\n",
    "\n",
    "        # Write decompressed text to a new file with .txt extension\n",
    "        with open(out_file_name, 'wb') as output_file:\n",
    "            output_file.write(decoded_text)\n",
    "\n",
    "\n",
    "    def deflate(self, text: str) -> bytes:\n",
    "        # Step 1: Huffman encoding\n",
    "        huffman_encoded, huffman_dict = self.huffman.encode(text)\n",
    "\n",
    "        # Step 2: LZ77 compression\n",
    "        lz77_nodes = self.lz77.encode_string(huffman_encoded)\n",
    "\n",
    "        # Combine Huffman dictionary and LZ77 nodes\n",
    "        combined_data = huffman_dict, lz77_nodes\n",
    "\n",
    "        # Serialize the combined data (e.g., using pickle or custom format)\n",
    "        # Return the serialized data as bytes\n",
    "        return combined_data\n",
    "\n",
    "    def huff_decode(self, code: str, coding_dict: dict[bytes, str]) -> bytes:\n",
    "        \"\"\"\n",
    "        Decodes files\n",
    "        \"\"\"\n",
    "        decoded_str = ''\n",
    "        coding_dict = {i : j for j, i in coding_dict.items()}\n",
    "        while code:\n",
    "            for cd in coding_dict:\n",
    "                if code.startswith(cd):\n",
    "                    decoded_str += coding_dict[cd]\n",
    "                    code = code[len(cd):]\n",
    "        return decoded_str\n",
    "\n",
    "    def inflate(self, compressed_data: bytes) -> str:\n",
    "        # Deserialize the combined data\n",
    "        huffman_dict, lz77_nodes = compressed_data\n",
    "\n",
    "        # Step 1: LZ77 decoding\n",
    "        huffman_encoded = self.lz77.decode_string(lz77_nodes)\n",
    "\n",
    "        # Step 2: Huffman decoding\n",
    "        decoded_text = self.huff_decode(huffman_encoded, huffman_dict)\n",
    "\n",
    "        return decoded_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other(data):\n",
    "    ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
